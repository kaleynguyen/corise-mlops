{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [MLOps] Week 2 starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaleynguyen/corise-mlops/blob/main/notebooks/Week_2_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZMsGNRLXi_e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "### Problem\n",
        "\n",
        "In the project this week, we will conduct detailed model evaluation, data quality testing and behavioral testing for the news classification model from the week 1 project.\n",
        "\n",
        "1. We will look at evaluation metrics beyond overall accuracy and f1 score (per-class metrics, confusion matrix, per-class false positive and false negative errors)\n",
        "2. We will implement bootstrap sampling to measure confidence intervals of model performance metrics\n",
        "3. We will implement behavioral tests using a popular open source library called checklist\n",
        "4. [advanced] Advanced data quality and behavioral tests\n",
        "\n",
        "### Deliverables\n",
        "1. Test set per-class accuracy, precision, recall and F-1 score\n",
        "2. Test set confusion matrix, and per-class false positive, false negative errors \n",
        "3. 95% confidence intervals for test set metrics as estimated via bootstrap sampling\n",
        "4. Behavioral tests for the trained news classification model, using checklist. \n",
        "5. [Optional] Advanced data quality and behavioral tests\n",
        "\n"
      ],
      "metadata": {
        "id": "u7Svh2auXQzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1: Prereqs & Installation\n",
        "\n",
        "Download & Import all the necessary libraries we need throughout the project."
      ],
      "metadata": {
        "id": "SaGcjyaNXW5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all the required dependencies for the project\n",
        "\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install sentence-transformers\n",
        "!pip install matplotlib\n",
        "!pip install great_expectations"
      ],
      "metadata": {
        "id": "TCU9I7qyXUHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package imports that will be needed for this project\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pprint import pprint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics as sklearn_metrics"
      ],
      "metadata": {
        "id": "4Huab9R3XZr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Constants\n",
        "\n",
        "LABEL_SET = [\n",
        "    'Business',\n",
        "    'Sci/Tech',\n",
        "    'Software and Developement',\n",
        "    'Entertainment',\n",
        "    'Sports',\n",
        "    'Health',\n",
        "    'Toons',\n",
        "    'Music Feeds'\n",
        "]\n",
        "\n",
        "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
        "DATA_URL = 'https://corise-mlops.s3.us-west-2.amazonaws.com/project2/agnews.zip'"
      ],
      "metadata": {
        "id": "qBHFKy2-Xbpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download and Load Dataset\n",
        "\n",
        "[AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) is a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. The news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626). The dataset has the text description (summary) of the news article along with some metadata. **For this project, we will use a slightly modified (cleaned up) version of this dataset** \n",
        "\n",
        "Schema:\n",
        "* Source - News publication source\n",
        "* URL - URL of the news article\n",
        "* Title - Title of the news article\n",
        "* Description - Summary description of the news article\n",
        "* Embedding - 768 dimensional embedding representation of the news article text (description) \n",
        "* Category (Label) - News category\n",
        "\n",
        "Sample row in this dataset:\n",
        "```\n",
        "{\n",
        "   'description': 'A capsule carrying solar material from the Genesis space probe has made a crash landing at a US Air Force training facility in the US state of Utah.',\n",
        "   'id': 86273,\n",
        "   'label': 'Entertainment',\n",
        "   'source': 'Voice of America',\n",
        "   'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
        "   'Embedding': [....],\n",
        "   'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "KUb4NSF1Xgra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "DIRECTORY_NAME = \"data\"\n",
        "\n",
        "\n",
        "def download_dataset():\n",
        "    \"\"\"\n",
        "    Download the dataset. The zip contains three files: train.json, test.json and unlabeled.json \n",
        "    \"\"\"\n",
        "    http_response = urlopen(DATA_URL)\n",
        "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
        "    zipfile.extractall(path=DIRECTORY_NAME)\n",
        "\n",
        "\n",
        "# Expensive operation so we should just do this once\n",
        "download_dataset()\n"
      ],
      "metadata": {
        "id": "g-9eqkvXXdu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Datasets = {}\n",
        "\n",
        "for ds in ['train', 'test']:\n",
        "    with open('data/{}.json'.format(ds), 'r') as f:\n",
        "        Datasets[ds] = json.load(f)\n",
        "    print(\"Loaded Dataset {0} with {1} rows\".format(ds, len(Datasets[ds])))\n",
        "\n",
        "print(\"\\nExample dataset row:\\n\")\n",
        "pprint(Datasets['train'][0])"
      ],
      "metadata": {
        "id": "vlrul_InXkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Quality Tests\n",
        "\n",
        "In Week 2 instruction, we learned about the importance of evaluating data quality across the ML lifecycle:\n",
        "* When building your first ML model, you often have no data and have to invest in annotation resources to get labels\n",
        "* Dataset labels can be noisy as they are collected from a variety of different sources.\n",
        "* Real world evolves (drift) and so should the datasets you’re using to train models (to make sure they evolve along with it). \n",
        "\n",
        "[Great Expectations](https://docs.greatexpectations.io/docs/) is a popular Python library for validating data in Machine Learning pipelines. It contains several built-in Expectation types to configure tests, as well as integrations with common data structures used to store datasets (e.g. Pandas dataframes, CSV files, MySQL databases etc). We will be using this library to define and run some data quality tests for our dataset. The goal and scope of the exercise here is to get you acquainted with the library and practice of testing for data quality.\n",
        "\n",
        "Useful references for this section:\n",
        "1. Documentation: https://docs.greatexpectations.io/docs/\n",
        "2. Using Great Expections in Jupyter: https://docs.greatexpectations.io/docs/guides/miscellaneous/how_to_quickly_explore_expectations_in_a_notebook \n",
        "3. List of predefined Expecations: https://greatexpectations.io/expectations/\n",
        "4. Creating custom Expecations for Pandas dataframes: https://github.com/great-expectations/great_expectations/blob/develop/docs_rtd/guides/how_to_guides/creating_and_editing_expectations/how_to_create_custom_expectations_for_pandas.rst "
      ],
      "metadata": {
        "id": "PrXQFXeHXoa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import great_expectations as ge\n",
        "from great_expectations.dataset import PandasDataset, MetaPandasDataset\n",
        "\n",
        "class AGNewsPandasDataset(PandasDataset):\n",
        "\n",
        "    _data_asset_type = \"AGNewsPandasDataset\"\n",
        "\n",
        "    @MetaPandasDataset.column_map_expectation\n",
        "    def expect_column_string_not_null_or_empty(self, column):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Write a custom expectation that checks that the string column's value for each\n",
        "        row is not NULL and not an empty string\n",
        "\n",
        "        Your code should look something like this:\n",
        "        column.map(lambda x: ...)\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    @MetaPandasDataset.column_map_expectation\n",
        "    def expect_column_embedding_dimensionality(self, column, dimension):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Write a custom expectation that checks that the embedding column's value for each\n",
        "        row is not NULL and dim(embedding) == dimension\n",
        "\n",
        "        Your code should look something like this:\n",
        "        column.map(lambda x: ...)\n",
        "        \"\"\"\n",
        "        return None\n",
        "    \n",
        "    @MetaPandasDataset.column_map_expectation\n",
        "    def expect_column_in_range(self, column, range_min, range_max):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Write a custom expectation that checks that the column's value for each\n",
        "        row is in range [range_min, range_max]\n",
        "\n",
        "        Your code should look something like this:\n",
        "        column.map(lambda x: ...)\n",
        "        \"\"\"\n",
        "        return None\n",
        "    \n",
        "    @MetaPandasDataset.column_map_expectation\n",
        "    def expect_column_embedding_unit_norm(self, column):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Write a custom expectation that checks that the embedding column's value for each\n",
        "        row is not NULL and norm(embedding) == 1\n",
        "\n",
        "        Your code should look something like this:\n",
        "        column.map(lambda x: ...)\n",
        "        \"\"\"\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "NqXtx4CpXmNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataframes = {}\n",
        "\n",
        "for ds in ['train', 'test']:\n",
        "    Dataframes[ds] = ge.from_pandas(\n",
        "        pd.DataFrame.from_records(Datasets[ds]), \n",
        "        dataset_class=AGNewsPandasDataset\n",
        "    )"
      ],
      "metadata": {
        "id": "UCRnbQTOXuwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ds in ['train', 'test']:\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Expect columns in Dataframes[ds] to match the column set: \n",
        "    {'id', 'source', 'title', 'url', 'rank', 'description', 'embedding', 'label'}\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Expect the \"id\" column in Dataframes[ds] to be unique\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Expect the \"description\" column in Dataframes[ds] not not be null or empty\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" \n",
        "    [TO BE IMPLEMENTED]\n",
        "\n",
        "    Expect the \"embedding\" column in Dataframes[ds] should be non-NULL and dimensionality = 768\n",
        "    (Run this after implementing `expect_column_embedding_dimensionality` above)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" \n",
        "    [TO BE IMPLEMENTED]\n",
        "\n",
        "    Expect the \"embedding\" column in Dataframes[ds] should be unit norm vectors\n",
        "    (Run this after implementing `expect_column_embedding_unit_norm` above)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" \n",
        "    [TO BE IMPLEMENTED]\n",
        "\n",
        "    Expect the \"rank\" column in Dataframes[ds] should be in range [1, 5]\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "dFjsWCPbXwf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_train = Dataframes[\"train\"].validate()\n",
        "\n",
        "pprint(validation_train)\n"
      ],
      "metadata": {
        "id": "Yz9fmvgxX0jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_test = Dataframes[\"test\"].validate()\n",
        "\n",
        "pprint(validation_test)\n"
      ],
      "metadata": {
        "id": "o0sNxBSeX1Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Take a look at the results from the data validation tests. You will observe that some rows in the \n",
        "training and test sets have failed the validation for the following test:\n",
        "\n",
        "Expect the \"description\" column in Dataframes[ds] not not be null or empty\n",
        "\n",
        "This is a good catch! We know that these rows in training or test set likely just add noise to the model\n",
        "since it is not very relevant to ask the model to predict the category of a news article with no text.\n",
        "\n",
        "In this step, you will filter out the rows from Datasets['train'] and Datasets['test']\n",
        "and create a new Datasets_filtered object with the filtered datasets\n",
        "\"\"\"\n",
        "\n",
        "Datasets_filtered = {}\n",
        "Datasets_filtered['train'] = []\n",
        "Datasets_filtered['test'] = []"
      ],
      "metadata": {
        "id": "VtLCXCGtX2ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Fine-grained Model Evaluation\n",
        "\n",
        "Model performance evaluation for real-world ML applications can often be complex since different stakeholders (ML engineers, product managers, sales teams) care about the impact on different dimensions. In this exercise, we will evaluate the News category classification from Week 1 in the following ways:\n",
        "\n",
        "\n",
        "* **Aggregate performance metrics**: Using standard metrics to evaluate ML model performance such as accuracy, mean squared error, BLEU scores etc (depending on the ML task) is a necessary first step, but far from sufficient. It serves to filter out models that are clearly suboptimal and reduces risk of launching bad models\n",
        "\n",
        "* **Slice-level performance metrics**: It is important to track model performance not just in aggregate but for important cohorts/slices of your traffic. For example, it is important to track the performance of your hate speech detection model not just in aggregate but for traffic from each country, language etc to understand the gaps in performance.\n",
        "  \n",
        "* **Qualitative evaluations**: Behavioral tests can be helpful to root-cause individual instances of model failures and yield helpful insights to improve models"
      ],
      "metadata": {
        "id": "gfoE5tB8YCax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to minimize variations from different model architectures & training procedures, \n",
        "# we will keep the model architecture and feature sets uniform\n",
        "\n",
        "X_train, Y_train = [], []\n",
        "X_test, Y_true = [], []\n",
        "\n",
        "for row in Datasets_filtered['train']:\n",
        "    X_train.append(row['embedding'])\n",
        "    Y_train.append(row['label'])\n",
        "\n",
        "for row in Datasets_filtered['test']:\n",
        "    X_test.append(row['embedding'])\n",
        "    Y_true.append(row['label'])\n",
        "\n",
        "model = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    tol=0.001,\n",
        "    solver='saga',\n",
        ")\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "DnvOr2MZYEUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This should be roughly in the same range as the results you got in Week 1\n",
        "\n",
        "print(\"Accuracy on test set: {}\".format(sklearn_metrics.accuracy_score(Y_true, Y_pred)))\n"
      ],
      "metadata": {
        "id": "aWjRTYDfYGl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 (Part 1): Confusion Matrix and Slice-based evaluation\n",
        "\n",
        "As we saw with issues discussed around rare classes and model bias, evaluating the performance metrics at the overall dataset level is often not enough to deploy in production. Here are some commonly used axes along which ML teams commonly slice their datasets:\n",
        "\n",
        "1. Labels (i.e. per-class performance)\n",
        "2. Key categorical features or buckets of continuous features. E.g. a recommender system’s performance for each gender, age group.\n",
        "3. Key traffic cohorts as defined by business value/regulatory requirements. E.g. a recommender system’s performance for newly registered users (it is important they have a good product experience else they will stop using the product)\n",
        "\n",
        "In this exercise, we will dig into per-class performance"
      ],
      "metadata": {
        "id": "wysCZrSgYIWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Plot a confusion matrix for the news category classification model trained above. \n",
        "You should have a NxN matrix M, where \n",
        "N = len(LABEL_SET)\n",
        "M(i, j) = number of instances in the test set where true_label = LABEL_SET[i] and pred_label = LABEL_SET[j]\n",
        "\n",
        "Hint: check out sklearn.metrics \n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "P1QSyb15YLHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Compute the precision, recall and F1 score per-class for the test set\n",
        "\n",
        "Hint: check out sklearn.metrics \n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "5FRR5csbYM5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "def confusion_matrix_errors(X, Y, preds, true_label, pred_label, num_errors=100):\n",
        "    \"\"\" \n",
        "    [TO BE IMPLEMENTED]\n",
        "\n",
        "    Write a function `confusion_matrix_errors` that accepts the following inputs:\n",
        "    1. The test dataset\n",
        "    2. True and predicted labels for each row in the dataset\n",
        "    3. A pair of labels (T, P)\n",
        "\n",
        "    And returns the `num_errors` no. of errors from the test dataset where true_label = T and pred_label = P\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    return random.sample(errors, num_errors)\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "[TO BE IMPLEMENTED]\n",
        "Use the function implemented above to get some insights about the kind of examples that the model\n",
        "is confused about for a given pair of classes\n",
        "Some pairs to try out: ('Entertainment', 'Sports'), ('Health', 'Sports'), ('Business', 'Sci/Tech')\n",
        "\"\"\"\n",
        "for e in confusion_matrix_errors(Datasets['test'], Y_true, Y_pred, 'Entertainment', 'Sports'):\n",
        "    print(e['description'])"
      ],
      "metadata": {
        "id": "OEXjTYqMYPJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 (Part 2): Estimating confidence intervals with Bootstrap sampling\n",
        "\n",
        "When computing model performance metrics, the margin of error can vary a lot depending on the size of the test dataset. We are trying to estimate the model’s performance on any unseen data by empirically computing the model performance on the held-out test set, and trying to quantify the confidence intervals in this estimation: If we were to measure the model’s performance on another independently collected test dataset from the same underlying distribution, our model’s performance on this dataset is unlikely to be the same, but how different might they plausibly be? \n",
        "\n",
        "In this exercise, we will implement Bootstrap sampling to estimate the 95% confidence interval for \n",
        "overall accuracy and F1-score of the model:\n",
        "\n",
        "1. Generate N ‘bootstrap sample’ datasets, each the same size as the original test set. Each bootstrap sample dataset is obtained by sampling instances uniformly at random from the original test set (with replacement).\n",
        "2. On each of the bootstrap sample datasets, calculate the accuracy metric.\n",
        "3. From Step (2), you will end up with N different accuracy values. Sort them.\n",
        "4. The 95% confidence interval is given by the 2.5th to the 97.5th percentile among the N sorted accuracy values.\n"
      ],
      "metadata": {
        "id": "0lXPDJZjYTNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_accuracy_distribution(Y_true, Y_pred, N = 1000):\n",
        "    \"\"\" \n",
        "    [TO BE IMPLEMENTED]\n",
        "    \n",
        "    Implement this function that takes two lists: Y_true, Y_pred\n",
        "    and the number of bootstrap trials (N). \n",
        "    \n",
        "    It should return a list (ret) of length = N, where ret[i] = accuracy metric from i-th bootstrap sampling run\n",
        "    \"\"\"\n",
        "    bootstrap_vals = [0]*N\n",
        "    return bootstrap_vals\n",
        "\n"
      ],
      "metadata": {
        "id": "GPt-rrOZYVDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "NUM_BOOTSTRAP = 1000\n",
        "\n",
        "bs_vals = bootstrap_accuracy_distribution(Y_true, Y_pred, NUM_BOOTSTRAP)\n",
        "bs_vals = sorted(bs_vals)\n",
        "\n",
        "print(\"95 percent confidence interval: [{0}, {1}]\".format(\n",
        "    bs_vals[25],    # 2.5th percentile\n",
        "    bs_vals[975]    # 97.5th percentile\n",
        "))\n",
        "\n",
        "_ = plt.hist(bs_vals, bins='auto')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-DGA1Q7TYWmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 (Part 3): Minimum Functionality behavioral Tests\n",
        "\n",
        "Unit tests and integration tests play an important role in testing software for bugs, inefficiencies and potential vulnerabilities. Can we employ a similar approach to testing ML models? This exercise introduces the concept of “behavioral testing”  for machine learning models. \n",
        "\n",
        "Minimum Functionality Tests are a class of behavioral tests, and equivalents of “unit tests in software engineering” - a collection of simple examples (and labels) to check a behavior. A recommended practice is to write minimum functionality tests for highly visible/high cost potential failure modes, and for failure modes that you uncover during error analysis to guard against such failures in the future. \n",
        "\n",
        "For this exercise, we will use a popular open source library called [Checklist](https://github.com/marcotcr/checklist) to configure and run behavioral tests for our model. The goal and scope of the exercise here is to get you acquainted with the library and practice of testing for minimum functionality. \n",
        "\n",
        "Useful references:\n",
        "1. Getting started with Checklist: https://github.com/marcotcr/checklist\n",
        "2. Creating & Running tests with Checklist: https://github.com/marcotcr/checklist/blob/9baab717e44e216697f7ef0730ee269db9ef7d5b/notebooks/tutorials/3.%20Test%20types,%20expectation%20functions,%20running%20tests.ipynb "
      ],
      "metadata": {
        "id": "edi_vbSrYbKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import checklist\n",
        "from checklist.editor import Editor\n",
        "from checklist.test_types import MFT\n",
        "from checklist.pred_wrapper import PredictorWrapper\n",
        "\n",
        "# Run some warmup code to get you familiar with templates in checlist\n",
        "editor = Editor()\n",
        "ret = editor.template('{first_name} is {a:profession} from {country}.',\n",
        "                      profession=['lawyer', 'doctor', 'accountant'])\n",
        "np.random.choice(ret.data, 3)"
      ],
      "metadata": {
        "id": "PsPlmzL2YcB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Part 1: Data Generation for behavioral tests\n",
        "\n",
        "1. News Source variation:\n",
        "{source}: Astronomers expect the Perseid meteor shower to be one of the best versions of the shooting star events in several years\n",
        "source = ['New York Times', 'Reuters', 'AP', 'Wall Street Journal', 'Quanta', 'BBC', 'BBC UK', 'Yahoo News']\n",
        "label = \"Sci/Tech\"\n",
        "\n",
        "2. Company Name variation:\n",
        "{company} revealed Thursday that its old recipe of adding stores is longer is a source of new profits for the company.\n",
        "company = ['McDonalds', 'Starbucks', 'Chipotle', 'Krispy Creme', 'Unknown Company']\n",
        "label = \"Sci/Tech\"\n",
        "\n",
        "3. Company Name variation:\n",
        "{company} revealed Thursday that its old recipe of adding stores is longer is a source of new profits for the company.\n",
        "company = ['McDonalds', 'Starbucks', 'Chipotle', 'Krispy Creme', 'Unknown Company']\n",
        "label = \"Sci/Tech\"\n",
        "\n",
        "4. \n",
        "{disease} will come under the {mask} during the charity gala event being held on Monday at 7pm\n",
        "disease = ['Breast cancer', 'Heart disease', 'Diabetes']\n",
        "\"\"\"\n",
        "\n",
        "editor = Editor()\n",
        "# [TO BE IMPLEMENTED]\n",
        "# ret = editor.template(...)\n",
        "# ret += editor.template(...)\n",
        "# ret += editor.template(...)\n",
        "\n"
      ],
      "metadata": {
        "id": "t5VZdy7nYfj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Part 2: Test Configuration\n",
        "\"\"\"\n",
        "\n",
        "test = MFT(**ret, name='AG News MFT tests')"
      ],
      "metadata": {
        "id": "_jcgRLWmYj6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Part 3: Test Run & Results summary\n",
        "\"\"\"\n",
        "\n",
        "def encode_and_predict(inputs):\n",
        "    encoder = SentenceTransformer(\n",
        "        'sentence-transformers/{model}'.format(model=EMBEDDING_MODEL)\n",
        "    )\n",
        "    encodings = [encoder.encode(x, normalize_embeddings=True) for x in inputs]\n",
        "    return model.predict(encodings)\n",
        "\n",
        "# wrapped_pp returns a tuple with (predictions, softmax confidences)\n",
        "wrapped_pp = PredictorWrapper.wrap_predict(encode_and_predict)\n",
        "test.run(wrapped_pp, overwrite=True)\n",
        "test.summary()"
      ],
      "metadata": {
        "id": "dqhzDdpnYlbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: [Optional] Advanced data quality and behavioral tests\n",
        "\n",
        "In the previous steps, you have familiarized yourself with libraries for writing data quality tests (great expectations) and behavioral tests (checkist). Take it out for a spin! A few possible extensions you can try:\n",
        "\n",
        "1. **Model invariance behavioral tests** -- in step 4, we wrote some minimum functionality tests. Can you design some useful invariance behavioral tests? \n",
        "\n",
        "2. **Embedding drift based data quality checks across Training/Test datasets** -- in step 3, we wrote some data quality tests. If we wanted to check for drift in embedding space between training and test data, how might we construct a data quality test for this? Hint: The distribution of euclidean distance between pairs of embeddings within the training, and between the training and test dataset should be “almost the same”\n",
        "\n"
      ],
      "metadata": {
        "id": "h7lNfekkYu8g"
      }
    }
  ]
}